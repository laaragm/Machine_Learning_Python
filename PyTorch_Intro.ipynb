{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-Intro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw684H5W077b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Resources: https://www.youtube.com/watch?reload=9&v=a4teH_l3q0c\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim #This is a package implementing various optimization algorithms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu9gxXAS1IZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98ea5cc1-50e9-4fac-deb0-e3583b84c47a"
      },
      "source": [
        "tensor = torch.Tensor([1,2,3])\n",
        "print(tensor)\n",
        "\n",
        "numpy_array = np.array([1,2,3])\n",
        "tensor2 = torch.from_numpy(numpy_array)\n",
        "print(tensor2)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkcqXtQh1vH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "90aff753-910e-4ef2-8b05-953353ac7a07"
      },
      "source": [
        "print(tensor.dot(tensor))\n",
        "print(tensor2 ** 2)\n",
        "print(tensor + 2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(14.)\n",
            "tensor([1, 4, 9])\n",
            "tensor([3., 4., 5.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1rHdu2B2A1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6558c977-af8f-4c19-9850-ba7581caa0d2"
      },
      "source": [
        "data = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
        "data.head(10)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-114.58</td>\n",
              "      <td>33.63</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1387.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>671.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>3.3438</td>\n",
              "      <td>74000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-114.58</td>\n",
              "      <td>33.61</td>\n",
              "      <td>25.0</td>\n",
              "      <td>2907.0</td>\n",
              "      <td>680.0</td>\n",
              "      <td>1841.0</td>\n",
              "      <td>633.0</td>\n",
              "      <td>2.6768</td>\n",
              "      <td>82400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-114.59</td>\n",
              "      <td>34.83</td>\n",
              "      <td>41.0</td>\n",
              "      <td>812.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>1.7083</td>\n",
              "      <td>48500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-114.59</td>\n",
              "      <td>33.61</td>\n",
              "      <td>34.0</td>\n",
              "      <td>4789.0</td>\n",
              "      <td>1175.0</td>\n",
              "      <td>3134.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>2.1782</td>\n",
              "      <td>58400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-114.60</td>\n",
              "      <td>34.83</td>\n",
              "      <td>46.0</td>\n",
              "      <td>1497.0</td>\n",
              "      <td>309.0</td>\n",
              "      <td>787.0</td>\n",
              "      <td>271.0</td>\n",
              "      <td>2.1908</td>\n",
              "      <td>48100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -114.31     34.19  ...         1.4936             66900.0\n",
              "1    -114.47     34.40  ...         1.8200             80100.0\n",
              "2    -114.56     33.69  ...         1.6509             85700.0\n",
              "3    -114.57     33.64  ...         3.1917             73400.0\n",
              "4    -114.57     33.57  ...         1.9250             65500.0\n",
              "5    -114.58     33.63  ...         3.3438             74000.0\n",
              "6    -114.58     33.61  ...         2.6768             82400.0\n",
              "7    -114.59     34.83  ...         1.7083             48500.0\n",
              "8    -114.59     33.61  ...         2.1782             58400.0\n",
              "9    -114.60     34.83  ...         2.1908             48100.0\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5mFXf4l2x2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop('median_house_value', axis=1)\n",
        "y = data['median_house_value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gI6S-GL3JRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module): #Our Net class is going to inherit from nn.Module (which is the base class for all neural network modules)\n",
        "  def __init__(self): #Constructor\n",
        "    super(Net, self).__init__() #Super keyword makes it possible for us to call the __init__ method from the parent class\n",
        "\n",
        "    #nn.Linear() is going to multiply each neuron by each weights and add the bias term\n",
        "    #Fully connected layers\n",
        "    #Here we are using 2 hidden layers but this is something you need to try and see what works best\n",
        "    self.fc1 = nn.Linear(in_features=8, out_features=100) #8 input features will be connected to other 100 on the first layer\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=100) #This second fully connected layer is going to get the previous 100 units and connect to other 100\n",
        "    self.output = nn.Linear(in_features=100, out_features=1) #This last layer will have 1 output neuron which is going to represent the house's predicted value\n",
        "\n",
        "  def forward_propagation(self, x):\n",
        "    #We are going to use a hidden layer to do the feature scaling process for us in order to make the process faster\n",
        "    x = F.normalize(x) #x is now our normalized feature array\n",
        "    x = F.relu(self.fc1(x)) #The Relu activation function will be applied to our first layer \n",
        "    x = F.relu(self.fc2(x)) #The Relu activation function will be applied to our second layer \n",
        "\n",
        "    y = self.output(x) #This is our output layer with the predictions\n",
        "\n",
        "    return y  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkKXkkL-dsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1b028a4d-91e8-45f0-c2e0-2a0e09156da8"
      },
      "source": [
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=8, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMVCJb3WAIUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tensor = torch.from_numpy(data.values[:, :-1]).float() \n",
        "y_tensor = torch.from_numpy(data.values[:, -1]).float()\n",
        "\n",
        "#Since our features have already been normalized, we have to do the same with our target \n",
        "y_tensor = torch.log(y_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNcj0xa-AWYQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca6ac013-8e84-4cd5-aedc-e83bb91a106b"
      },
      "source": [
        "#net.parameters(): matrices of the weights which connects the neurons of our Neural Network\n",
        "list(net.parameters())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0933,  0.0148,  0.2695,  0.2668,  0.2264,  0.2147,  0.0171,  0.1570],\n",
              "         [ 0.2298,  0.1852,  0.2017, -0.2311, -0.1907,  0.1726, -0.0300,  0.2542],\n",
              "         [-0.2397, -0.0038, -0.2607, -0.2091,  0.0394, -0.3170, -0.1863, -0.2380],\n",
              "         [ 0.2346,  0.0432, -0.1587, -0.3360, -0.2367,  0.1873, -0.0891,  0.2626],\n",
              "         [ 0.0476,  0.0260,  0.1953, -0.2039,  0.1932,  0.1339, -0.0006,  0.1404],\n",
              "         [-0.2298,  0.2012, -0.3045,  0.0469,  0.3361, -0.0681, -0.0759,  0.2930],\n",
              "         [ 0.2537, -0.0025,  0.2493,  0.2553, -0.0875,  0.1657, -0.1190,  0.1767],\n",
              "         [-0.3030, -0.2319,  0.1101,  0.3287, -0.2544,  0.3131,  0.2389, -0.3001],\n",
              "         [ 0.0713,  0.1440, -0.3362, -0.2107,  0.2271,  0.2871, -0.1209, -0.0161],\n",
              "         [-0.0994, -0.2009, -0.2164,  0.1614,  0.2313, -0.2359, -0.2912, -0.2842],\n",
              "         [-0.3509, -0.3470,  0.1452, -0.1363,  0.0423,  0.2872, -0.1846,  0.1149],\n",
              "         [-0.0165,  0.0920,  0.2973, -0.1023, -0.0209, -0.2857, -0.1160,  0.2492],\n",
              "         [ 0.3181,  0.2337, -0.0360,  0.2496,  0.3335,  0.0578,  0.2931, -0.1022],\n",
              "         [-0.1446, -0.2497, -0.0700, -0.1140,  0.2013, -0.2389, -0.2364, -0.1282],\n",
              "         [-0.3416,  0.1174,  0.2997,  0.2586, -0.1798,  0.2401, -0.2550, -0.1446],\n",
              "         [-0.2875,  0.3043, -0.1511, -0.3314, -0.0737,  0.2957,  0.1686,  0.2863],\n",
              "         [ 0.2381, -0.0596,  0.2847, -0.1739,  0.2441, -0.2757, -0.2074,  0.3284],\n",
              "         [-0.0883,  0.1868,  0.2139, -0.1762,  0.2594,  0.2050,  0.1815,  0.1643],\n",
              "         [ 0.2411, -0.2970,  0.2655,  0.0992,  0.0908,  0.3272, -0.2187,  0.3310],\n",
              "         [-0.1067, -0.1529, -0.3096,  0.0921, -0.0726, -0.3206,  0.1066, -0.2371],\n",
              "         [ 0.3158,  0.2238,  0.2818,  0.0293, -0.0948,  0.0122,  0.3225, -0.0810],\n",
              "         [-0.0751, -0.0566, -0.2780, -0.2648, -0.0098, -0.1949,  0.0882,  0.3420],\n",
              "         [-0.2289, -0.0711, -0.2661, -0.1878,  0.2403,  0.2662, -0.2972, -0.0485],\n",
              "         [ 0.2039,  0.2026,  0.3355, -0.2484, -0.1502, -0.0658,  0.0915,  0.0065],\n",
              "         [ 0.0417, -0.0278,  0.1424, -0.0702, -0.3268, -0.2228,  0.1016, -0.3136],\n",
              "         [-0.3092,  0.2870, -0.2616, -0.2437,  0.3321, -0.1418, -0.2239, -0.2037],\n",
              "         [ 0.3229,  0.1661,  0.0043,  0.2551, -0.0933, -0.3167,  0.3062, -0.0259],\n",
              "         [-0.1324, -0.0530,  0.3280,  0.3212, -0.1985, -0.3138,  0.0439,  0.0284],\n",
              "         [-0.2220,  0.0598,  0.1481, -0.2853,  0.2903, -0.1029,  0.2779, -0.1035],\n",
              "         [-0.0069, -0.1666, -0.3060, -0.2262, -0.3331,  0.2519,  0.3122,  0.0584],\n",
              "         [-0.0199,  0.1453, -0.0803, -0.0457, -0.2989, -0.2000,  0.2626,  0.3158],\n",
              "         [ 0.2302, -0.2147, -0.0702,  0.1642, -0.3240,  0.3122,  0.3490, -0.3415],\n",
              "         [ 0.1369, -0.1911,  0.1920,  0.3417, -0.2844, -0.1946,  0.2368,  0.2645],\n",
              "         [-0.0732,  0.0490, -0.2431,  0.2524,  0.2327, -0.0303, -0.1968, -0.2576],\n",
              "         [-0.0616,  0.0124, -0.1884,  0.3360,  0.0589, -0.3246, -0.2839, -0.0964],\n",
              "         [-0.1797, -0.3140, -0.2250,  0.0380,  0.1194, -0.2042, -0.1280, -0.0634],\n",
              "         [ 0.1816,  0.3016, -0.2847, -0.2193, -0.0971,  0.3025,  0.1572, -0.2709],\n",
              "         [ 0.1532, -0.1295, -0.0048,  0.0553,  0.3087,  0.0412, -0.3185, -0.1350],\n",
              "         [ 0.0617,  0.1071, -0.0263,  0.1942,  0.2187,  0.0321,  0.3145, -0.1591],\n",
              "         [ 0.1339,  0.1165,  0.1064, -0.2713, -0.1056, -0.1327, -0.1573, -0.0486],\n",
              "         [ 0.3265,  0.2467,  0.3455,  0.1008, -0.2428, -0.1776, -0.0676,  0.2551],\n",
              "         [ 0.1198, -0.0665,  0.2683, -0.1440,  0.0685, -0.3299,  0.0411, -0.3470],\n",
              "         [ 0.3114,  0.0366, -0.2990, -0.0884, -0.0151,  0.2634, -0.2844,  0.3463],\n",
              "         [ 0.3417,  0.1621, -0.0712,  0.3352,  0.2941,  0.0050,  0.0773,  0.2676],\n",
              "         [-0.2246,  0.2802, -0.1920, -0.1594, -0.3164,  0.0033,  0.2738,  0.2613],\n",
              "         [-0.2244, -0.1709, -0.1169,  0.3091,  0.0991,  0.0361,  0.0406, -0.2112],\n",
              "         [ 0.2053, -0.1421, -0.1127, -0.0819,  0.2231, -0.0536, -0.0408, -0.2082],\n",
              "         [-0.2700,  0.0997, -0.1592, -0.3527, -0.2445,  0.0730, -0.0266,  0.2266],\n",
              "         [-0.0176,  0.2754, -0.2110,  0.1390, -0.1395, -0.2272, -0.2431,  0.2340],\n",
              "         [ 0.1664, -0.1873, -0.0519, -0.1157,  0.0097,  0.2866, -0.1144,  0.2605],\n",
              "         [ 0.1283,  0.2457,  0.3162, -0.0118,  0.0157, -0.2959,  0.1753, -0.3383],\n",
              "         [ 0.2471,  0.2123,  0.2831, -0.0366,  0.2560, -0.2091, -0.1541, -0.1378],\n",
              "         [-0.3060, -0.0967, -0.3057,  0.1572, -0.1382, -0.2122,  0.1072,  0.0903],\n",
              "         [-0.0501, -0.0176,  0.0672, -0.1113, -0.0902, -0.0202, -0.3416,  0.1573],\n",
              "         [-0.2623,  0.2981,  0.2085,  0.2561,  0.1458,  0.0788, -0.3093,  0.1486],\n",
              "         [ 0.1365, -0.3234,  0.3366, -0.0673, -0.2775, -0.0034, -0.1862, -0.2684],\n",
              "         [ 0.1678,  0.2523,  0.0704,  0.1258, -0.3122,  0.0525, -0.1661, -0.2790],\n",
              "         [-0.1087, -0.3366, -0.0826, -0.0142,  0.1071, -0.1318,  0.2241, -0.1784],\n",
              "         [ 0.2163,  0.1621, -0.2119,  0.0489, -0.0050,  0.0592,  0.0605, -0.0935],\n",
              "         [-0.2705, -0.1016,  0.2935,  0.2710,  0.0695,  0.1774, -0.1517,  0.0260],\n",
              "         [ 0.0785,  0.2582,  0.1956, -0.2095,  0.1025, -0.1543, -0.2557, -0.0395],\n",
              "         [ 0.1577,  0.1660,  0.1884, -0.2314,  0.1410,  0.0447,  0.2356,  0.2965],\n",
              "         [-0.2118,  0.0236, -0.0442,  0.2533,  0.2887,  0.0376,  0.0502,  0.3220],\n",
              "         [-0.1815,  0.3512, -0.0561, -0.0283, -0.2661, -0.1821,  0.0105, -0.2560],\n",
              "         [-0.3107, -0.0805,  0.3455,  0.2315,  0.3022, -0.2116, -0.1034, -0.2435],\n",
              "         [-0.2341, -0.0108, -0.1718, -0.1516, -0.2860,  0.3524, -0.0864, -0.3351],\n",
              "         [ 0.1230, -0.0304, -0.3204, -0.3127, -0.3093,  0.1169, -0.1361, -0.1525],\n",
              "         [-0.2331,  0.0738, -0.1976, -0.1847,  0.1111, -0.0586,  0.0423, -0.0349],\n",
              "         [-0.2142,  0.1350,  0.2291,  0.0213, -0.2600, -0.1636, -0.0303, -0.0191],\n",
              "         [-0.1660, -0.2200, -0.3283,  0.1375, -0.3470,  0.2222, -0.0916,  0.3395],\n",
              "         [ 0.0228, -0.2299,  0.2080,  0.0356, -0.3015, -0.0957, -0.3097,  0.2358],\n",
              "         [-0.1038, -0.0478,  0.1317,  0.1548, -0.2661,  0.0222, -0.0293,  0.2600],\n",
              "         [-0.3356, -0.2379,  0.2370, -0.2007,  0.1061, -0.1098, -0.3256,  0.0202],\n",
              "         [ 0.3264,  0.0507,  0.3475,  0.0899,  0.1682, -0.1194,  0.0700,  0.1758],\n",
              "         [-0.2649,  0.3080,  0.0528, -0.2037, -0.0789, -0.2385,  0.0949,  0.0854],\n",
              "         [ 0.1916,  0.0839, -0.3163, -0.0938, -0.3196, -0.0683,  0.1412, -0.1834],\n",
              "         [ 0.0424,  0.3240, -0.1325,  0.0916,  0.2940, -0.0029,  0.1709,  0.2254],\n",
              "         [ 0.0520,  0.0027,  0.0665, -0.3453, -0.0093, -0.1282,  0.3093,  0.1394],\n",
              "         [ 0.0884,  0.2627, -0.2152,  0.1879,  0.3352,  0.0394,  0.1789, -0.3364],\n",
              "         [ 0.3058, -0.0810,  0.0330,  0.2674,  0.2755,  0.2969,  0.3241,  0.1493],\n",
              "         [ 0.0991,  0.1590,  0.0323,  0.1200, -0.2114, -0.0366, -0.2557,  0.2129],\n",
              "         [ 0.0982,  0.3331,  0.2998, -0.1412,  0.1898,  0.2234,  0.0115, -0.1326],\n",
              "         [ 0.3366,  0.0858,  0.0819,  0.1411, -0.1860, -0.2431, -0.0972,  0.3532],\n",
              "         [-0.1147, -0.0641, -0.1075, -0.3385,  0.0590, -0.0223,  0.3376,  0.2757],\n",
              "         [ 0.0063,  0.2520,  0.1845,  0.1901, -0.2100,  0.1751, -0.2068, -0.2267],\n",
              "         [-0.1065, -0.2394, -0.1739, -0.2453,  0.0060, -0.3497, -0.0298, -0.1146],\n",
              "         [ 0.1833,  0.0179, -0.1940,  0.2661, -0.3501, -0.1271,  0.3158,  0.2871],\n",
              "         [-0.1219,  0.2977, -0.2282, -0.2142, -0.0503, -0.2814,  0.0322, -0.1858],\n",
              "         [-0.1597,  0.1248, -0.3079,  0.2366,  0.3161, -0.0731,  0.1923, -0.2619],\n",
              "         [-0.2627, -0.0158, -0.1171,  0.1598,  0.1129,  0.0689,  0.3101,  0.1404],\n",
              "         [ 0.3418,  0.2868, -0.2692,  0.1212, -0.1995, -0.2899,  0.2049,  0.0630],\n",
              "         [-0.1794,  0.0095, -0.0981, -0.1936, -0.0913, -0.3499, -0.3352,  0.2079],\n",
              "         [-0.3515, -0.2167, -0.1279,  0.2894,  0.1150, -0.1208,  0.2711, -0.1226],\n",
              "         [ 0.2642,  0.0216, -0.2287,  0.3064,  0.3229, -0.1192,  0.1133,  0.1651],\n",
              "         [ 0.1845, -0.0724, -0.0659,  0.1171,  0.1435,  0.1472,  0.1800,  0.0089],\n",
              "         [ 0.0156,  0.2834, -0.3026,  0.1067, -0.0160, -0.2733,  0.1252, -0.2672],\n",
              "         [-0.3206,  0.3244,  0.2033,  0.0100, -0.2848, -0.0260, -0.3252,  0.0861],\n",
              "         [-0.0513, -0.1824, -0.2706, -0.1813,  0.1825, -0.0699,  0.1063, -0.0265],\n",
              "         [-0.1367, -0.1060, -0.1979,  0.1647,  0.1852,  0.1373, -0.1134,  0.0194],\n",
              "         [-0.0592,  0.0262, -0.0551, -0.0156, -0.0688,  0.0092, -0.2456,  0.2661]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0216, -0.2207, -0.3098, -0.1699, -0.0735, -0.2403,  0.0685,  0.2025,\n",
              "         -0.0582,  0.1470, -0.1709,  0.0841,  0.2000, -0.3168, -0.1161, -0.2325,\n",
              "         -0.0998,  0.1188,  0.1886, -0.1455, -0.3099, -0.2454, -0.0199,  0.3532,\n",
              "          0.1866,  0.1213, -0.1314, -0.1161, -0.3499, -0.2264,  0.2453, -0.2192,\n",
              "         -0.2515,  0.3010, -0.1372,  0.2058, -0.2887, -0.0106,  0.0622,  0.2128,\n",
              "         -0.0535, -0.1671,  0.0036,  0.0114, -0.0713, -0.3284, -0.1346, -0.2338,\n",
              "         -0.2542, -0.3230,  0.0919, -0.1975,  0.1449, -0.1139,  0.3454, -0.2964,\n",
              "          0.3461, -0.0942, -0.0120,  0.2423,  0.1770,  0.0801, -0.2200,  0.1767,\n",
              "         -0.0817, -0.0454, -0.1679,  0.2953, -0.3254,  0.0698,  0.2496, -0.2062,\n",
              "          0.2935, -0.0084, -0.0525,  0.2446,  0.2700, -0.3093,  0.2925,  0.0392,\n",
              "         -0.0451,  0.0978, -0.2191, -0.3021, -0.0780,  0.0359,  0.2062, -0.0670,\n",
              "          0.1587,  0.0494, -0.2318, -0.2958,  0.0313,  0.0784,  0.3303,  0.2876,\n",
              "         -0.0682, -0.0965, -0.3016, -0.0062], requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0471,  0.0980, -0.0507,  ..., -0.0960,  0.0841, -0.0086],\n",
              "         [ 0.0718, -0.0818, -0.0682,  ..., -0.0561, -0.0955, -0.0898],\n",
              "         [-0.0315, -0.0834,  0.0450,  ..., -0.0089,  0.0670, -0.0583],\n",
              "         ...,\n",
              "         [-0.0476, -0.0771,  0.0294,  ..., -0.0770, -0.0335, -0.0386],\n",
              "         [ 0.0844,  0.0121, -0.0688,  ...,  0.0838, -0.0275,  0.0697],\n",
              "         [ 0.0735, -0.0745, -0.0705,  ...,  0.0477, -0.0510, -0.0973]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 4.6194e-02,  3.1795e-02, -8.8793e-02,  2.3615e-02, -5.6466e-02,\n",
              "          5.2358e-04, -8.7290e-02,  7.0074e-02,  9.0876e-02,  8.5477e-02,\n",
              "         -6.6666e-02, -9.7269e-02, -1.9942e-02, -7.7074e-02, -5.2484e-02,\n",
              "         -5.1359e-02, -4.0681e-02, -9.5767e-02, -9.5362e-03, -2.3669e-02,\n",
              "         -6.2675e-02,  8.8615e-03, -4.2023e-02,  4.9199e-02,  9.6546e-02,\n",
              "         -4.8861e-02,  7.3539e-02,  3.0618e-03,  9.1360e-02, -3.1849e-02,\n",
              "         -5.8699e-02,  4.0816e-02, -3.1284e-02, -7.0473e-03,  6.3236e-02,\n",
              "          5.8787e-02,  6.4269e-02,  9.7618e-02, -8.2422e-03,  7.7504e-02,\n",
              "         -5.2903e-02,  9.3118e-02, -6.5273e-02, -4.9623e-03,  3.5537e-02,\n",
              "          3.8276e-02,  1.8770e-02, -8.1025e-02,  2.8547e-02,  6.9091e-02,\n",
              "          3.1143e-02, -2.0061e-02,  2.3122e-02,  5.8314e-02,  9.2095e-02,\n",
              "          3.1801e-02,  8.9876e-02, -2.2477e-03,  6.9028e-02,  5.5935e-02,\n",
              "          9.8880e-02, -1.2273e-02,  8.1428e-02, -8.8207e-03, -5.7333e-02,\n",
              "         -4.9156e-02, -1.5008e-02, -7.2355e-02, -3.6294e-02, -8.3875e-02,\n",
              "          5.4327e-02, -1.4065e-02,  2.3515e-02,  9.2668e-02, -6.6899e-02,\n",
              "          9.0136e-02, -5.4280e-02, -1.7636e-02, -8.7979e-02, -5.2600e-02,\n",
              "         -3.7167e-02,  8.3822e-02,  8.5423e-02, -3.0357e-02, -6.3098e-02,\n",
              "         -6.9286e-02,  9.6882e-03,  2.3232e-02, -6.0989e-02,  4.9050e-02,\n",
              "          6.2889e-02, -4.0522e-02, -8.6850e-02,  3.3057e-02, -1.6493e-02,\n",
              "         -7.9969e-02,  5.8964e-05,  7.9656e-02,  4.7917e-02, -5.2625e-02],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[-0.0747,  0.0478,  0.0823,  0.0359,  0.0017,  0.0908, -0.0123, -0.0224,\n",
              "          -0.0395, -0.0772,  0.0449, -0.0536, -0.0049, -0.0279,  0.0483, -0.0523,\n",
              "           0.0729, -0.0207,  0.0159, -0.0243,  0.0717,  0.0227, -0.0232, -0.0846,\n",
              "          -0.0934,  0.0604,  0.0301,  0.0586,  0.0066, -0.0389,  0.0414,  0.0064,\n",
              "           0.0554,  0.0091,  0.0251,  0.0093, -0.0745, -0.0988, -0.0531, -0.0300,\n",
              "           0.0280, -0.0214, -0.0798,  0.0851,  0.0837, -0.0076,  0.0406,  0.0005,\n",
              "          -0.0245,  0.0732,  0.0627, -0.0550,  0.0446,  0.0685, -0.0855, -0.0596,\n",
              "           0.0299, -0.0238,  0.0204,  0.0386, -0.0285, -0.0661,  0.0737,  0.0865,\n",
              "          -0.0317, -0.0617,  0.0294,  0.0412,  0.0576, -0.0846, -0.0454, -0.0704,\n",
              "           0.0759,  0.0839, -0.0405,  0.0680,  0.0664,  0.0799, -0.0738, -0.0801,\n",
              "          -0.0209,  0.0086,  0.0952, -0.0440,  0.0536,  0.0070,  0.0520, -0.0067,\n",
              "           0.0790,  0.0988,  0.0462, -0.0649,  0.0037,  0.0113, -0.0858, -0.0758,\n",
              "           0.0890,  0.0564,  0.0728, -0.0419]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.0506], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLklpS3g_utv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHGimMRmBK39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In order to check whether the optimizer is good, we are going to evaluate it using RMSE \n",
        "criterion = nn.MSELoss() #The main goal is to reduce the error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9oKGfBQB8ir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68ee06e5-a7cc-4752-fb61-36e1a679072f"
      },
      "source": [
        "for i in range(100):\n",
        "  #In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward\n",
        "  #passes. This is convenient while training RNNs. Because of this, when you start your training loop, ideally you should zero out the gradients so that you \n",
        "  #do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum.\n",
        "  optimizer.zero_grad() #Set optimizer gradient functions equal to zero\n",
        "  prediction = net.forward_propagation(X_tensor) #Here we are using the 'net' object with the tensor containing the data we want to train  \n",
        "  loss = criterion(prediction, y_tensor)\n",
        "  #backward() is going to calculate all the gradients so then the optimizer might use them by adjusting the weights in order to minimize our function\n",
        "  loss.backward()\n",
        "  optimizer.step() #Performs a parameter update based on the current gradient \n",
        "  \n",
        "  prediction2 = torch.exp(net.forward_propagation(X_tensor))\n",
        "  loss = criterion(prediction2, torch.exp(y_tensor))\n",
        "  print('Monitoring the loss: ', torch.sqrt(loss))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([17000])) that is different to the input size (torch.Size([17000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Monitoring the loss:  tensor(127059.6406, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126999.9297, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126941.6406, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126882.4531, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126825.8984, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126767.9609, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126709.9141, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126653.3750, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126595.8359, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126538.0625, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126481.7188, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126424.3984, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126364.6328, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126307.5781, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126250.5781, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126192.8047, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126135.0859, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126077.5078, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(126019.9531, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125964.0859, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125907.8984, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125852.5469, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125794.6719, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125739.8516, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125684.2891, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125628.3516, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125573.4688, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125518.7734, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125463.1094, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125409.1328, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125354.7500, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125302.4531, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125249.5234, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125196.5703, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125144.3125, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125091.4062, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(125040.0781, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124987.9766, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124934.7812, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124881.9062, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124828.5234, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124776.7656, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124724.3984, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124671.7422, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124618.6875, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124566.4453, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124513.1250, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124460.5859, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124407.6562, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124355.9219, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124305.0078, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124252.6250, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124201.6328, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124150.6328, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124099.9062, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124049.7031, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(124000.1719, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123952.2734, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123904.3984, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123857.7891, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123809.8516, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123762.9922, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123717.2734, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123672.5156, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123628.3828, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123584.8594, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123540.0859, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123496.6016, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123454.4375, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123413.1719, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123370.8750, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123329.2969, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123285.8281, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123243.4531, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123201.8516, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123161.1953, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123120.4375, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123079.2891, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(123038.6953, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122997.1328, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122957.8516, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122917.6719, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122878.4219, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122838.0938, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122800.3672, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122761.3906, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122721.5547, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122683.2734, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122645.1406, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122605.9844, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122567.9688, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122528.8906, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122491.5078, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122456.3750, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122419.9844, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122384.4688, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122348.3516, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122313.7812, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122278.5938, grad_fn=<SqrtBackward>)\n",
            "Monitoring the loss:  tensor(122243.7578, grad_fn=<SqrtBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKemfJEwP78U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f43ca711-7830-4b6d-a9db-43e0695243c7"
      },
      "source": [
        "test_df = pd.read_csv(\"sample_data/california_housing_test.csv\")\n",
        "test_df.head(10)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-119.56</td>\n",
              "      <td>36.51</td>\n",
              "      <td>37.0</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>663.0</td>\n",
              "      <td>204.0</td>\n",
              "      <td>1.6635</td>\n",
              "      <td>67000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-121.43</td>\n",
              "      <td>38.63</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1009.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>604.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>1.6641</td>\n",
              "      <td>67000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-120.65</td>\n",
              "      <td>35.48</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2310.0</td>\n",
              "      <td>471.0</td>\n",
              "      <td>1341.0</td>\n",
              "      <td>441.0</td>\n",
              "      <td>3.2250</td>\n",
              "      <td>166900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-122.84</td>\n",
              "      <td>38.40</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3080.0</td>\n",
              "      <td>617.0</td>\n",
              "      <td>1446.0</td>\n",
              "      <td>599.0</td>\n",
              "      <td>3.6696</td>\n",
              "      <td>194400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-118.02</td>\n",
              "      <td>34.08</td>\n",
              "      <td>31.0</td>\n",
              "      <td>2402.0</td>\n",
              "      <td>632.0</td>\n",
              "      <td>2830.0</td>\n",
              "      <td>603.0</td>\n",
              "      <td>2.3333</td>\n",
              "      <td>164200.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -122.05     37.37  ...         6.6085            344700.0\n",
              "1    -118.30     34.26  ...         3.5990            176500.0\n",
              "2    -117.81     33.78  ...         5.7934            270500.0\n",
              "3    -118.36     33.82  ...         6.1359            330000.0\n",
              "4    -119.67     36.33  ...         2.9375             81700.0\n",
              "5    -119.56     36.51  ...         1.6635             67000.0\n",
              "6    -121.43     38.63  ...         1.6641             67000.0\n",
              "7    -120.65     35.48  ...         3.2250            166900.0\n",
              "8    -122.84     38.40  ...         3.6696            194400.0\n",
              "9    -118.02     34.08  ...         2.3333            164200.0\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTzMManITUF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_tensor = torch.from_numpy(test_df.values[:, :-1]).float()\n",
        "y_test = test_df.values[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7KPSS7lTZ17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "229b6886-298c-4c7d-a5ba-232f2f26da4a"
      },
      "source": [
        "prediction_test = torch.exp(net.forward_propagation(X_test_tensor))\n",
        "print(prediction_test)\n",
        "\n",
        "prediction_test_numpy = prediction_test.detach().numpy()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[160062.9062],\n",
            "        [188834.1562],\n",
            "        [148795.7500],\n",
            "        ...,\n",
            "        [192273.7812],\n",
            "        [248282.2031],\n",
            "        [168334.3750]], grad_fn=<ExpBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yME2vunYUIJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec52542d-0bed-4f69-a116-b4baf3e12a94"
      },
      "source": [
        "y_test = test_df.values[:, -1]\n",
        "np.sqrt(mean_squared_error(y_test, prediction_test_numpy))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123668.57102607541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    }
  ]
}